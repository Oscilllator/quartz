<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="n body is not so trivial [[20230507 Simulation|Before]] I said that an n body simulation would be trivial. But it turns out that it might be quite so much as I had thought."><title>Harry dB notes</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://Oscilllator.github.io/quartz//icon.png><link href=https://Oscilllator.github.io/quartz/styles.708c2658f93e3a9d323a1f9fded8f4b2.min.css rel=stylesheet><link href=https://Oscilllator.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://Oscilllator.github.io/quartz/js/darkmode.9b46d81b9b161bfac149d66dfa6b3812.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://Oscilllator.github.io/quartz/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://Oscilllator.github.io/quartz/",fetchData=Promise.all([fetch("https://Oscilllator.github.io/quartz/indices/linkIndex.f662edd8cb6283b9dec6c7bb9174420d.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://Oscilllator.github.io/quartz/indices/contentIndex.a28afba11671bbd1d4488fecd7841c14.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://Oscilllator.github.io/quartz",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://Oscilllator.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/Oscilllator.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://Oscilllator.github.io/quartz/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://Oscilllator.github.io/quartz/>Harry dB notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/20230604%20Simulate%20trivialities.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><a href=#state>State</a></li><li><a href=#initial-attempt>Initial attempt</a><ol><li><a href=#gcn>GCN</a></li><li><a href=#fully-connected-mlp>Fully connected MLP</a></li></ol></li></ol><ol><li><a href=#the-bitter-lesson>The bitter lesson</a><ol><li><a href=#small-experiments-adding-spiciness>Small experiments adding spiciness</a></li><li><a href=#inverse>Inverse</a></li></ol></li></ol></nav></details></aside><a href=#n-body-is-not-so-trivial><h1 id=n-body-is-not-so-trivial><span class=hanchor arialabel=Anchor># </span>n body is not so trivial</h1></a><p><a href=/quartz/20230507-Simulation rel=noopener class=internal-link data-src=/quartz/20230507-Simulation>Before</a> I said that an n body simulation would be trivial. But it turns out that it might be quite so much as I had thought. I wrote a simple n body simulation:</p><p><img src="/quartz/Screencast from 06-04-2023 075012 AM.webm" width=auto>
That whilst not particularly accurate because of floating point shenanigans, nonetheless behaves in a predictable way.</p><a href=#state><h2 id=state><span class=hanchor arialabel=Anchor># </span>State</h2></a><p>The state of each particle is a 5 dimensional vector - [mass, x, y, vx, vy]. So a 10-particle system is a 10x5 matrix.</p><a href=#initial-attempt><h2 id=initial-attempt><span class=hanchor arialabel=Anchor># </span>Initial attempt</h2></a><p>I initially copy/pasted hacked together a trivial Graph convolutional neural net using the pytorch geometric framework:</p><a href=#gcn><h3 id=gcn><span class=hanchor arialabel=Anchor># </span>GCN</h3></a><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span><span class=lnt>28
</span><span class=lnt>29
</span><span class=lnt>30
</span><span class=lnt>31
</span><span class=lnt>32
</span><span class=lnt>33
</span><span class=lnt>34
</span><span class=lnt>35
</span><span class=lnt>36
</span><span class=lnt>37
</span><span class=lnt>38
</span><span class=lnt>39
</span><span class=lnt>40
</span><span class=lnt>41
</span><span class=lnt>42
</span><span class=lnt>43
</span><span class=lnt>44
</span><span class=lnt>45
</span><span class=lnt>46
</span><span class=lnt>47
</span><span class=lnt>48
</span><span class=lnt>49
</span><span class=lnt>50
</span><span class=lnt>51
</span><span class=lnt>52
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GCNConv</span><span class=p>(</span><span class=n>MessagePassing</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>(</span><span class=n>aggr</span><span class=o>=</span><span class=s1>&#39;add&#39;</span><span class=p>)</span>  <span class=c1># &#34;Add&#34; aggregation (Step 5).</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>project_in</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>in_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=c1># self.edge_lin = Linear(2 * out_channels, out_channels)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>message_join</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>bias</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>edge_mlp</span> <span class=o>=</span> <span class=n>Seq</span><span class=p>(</span><span class=n>Linear</span><span class=p>(</span><span class=mi>2</span> <span class=o>*</span> <span class=n>out_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>),</span>
</span></span><span class=line><span class=cl>                       <span class=n>ReLU</span><span class=p>(),</span>
</span></span><span class=line><span class=cl>                       <span class=n>Linear</span><span class=p>(</span><span class=n>out_channels</span><span class=p>,</span> <span class=n>out_channels</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>reset_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>reset_parameters</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>project_in</span><span class=o>.</span><span class=n>reset_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>message_join</span><span class=o>.</span><span class=n>reset_parameters</span><span class=p>()</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>,</span> <span class=n>edge_index</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>project_in</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Step 4-5: Start propagating messages.</span>
</span></span><span class=line><span class=cl>        <span class=n>messages</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>propagate</span><span class=p>(</span><span class=n>edge_index</span><span class=p>,</span> <span class=n>x</span><span class=o>=</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>out</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>message_join</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>messages</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>out</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>message</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x_i</span><span class=p>,</span> <span class=n>x_j</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>tmp</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>([</span><span class=n>x_i</span><span class=p>,</span> <span class=n>x_j</span> <span class=o>-</span> <span class=n>x_i</span><span class=p>],</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># tmp has shape [E, 2 * in_channels]</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=bp>self</span><span class=o>.</span><span class=n>edge_mlp</span><span class=p>(</span><span class=n>tmp</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>class</span> <span class=nc>GCN</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_features</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_size</span> <span class=o>=</span> <span class=n>nbody</span><span class=o>.</span><span class=n>NUM_STATES</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span> <span class=o>=</span> <span class=n>GCNConv</span><span class=p>(</span><span class=n>num_features</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv1</span> <span class=o>=</span> <span class=n>GCNConv</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv2</span> <span class=o>=</span> <span class=n>GCNConv</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># self.output_conv = GCNConv(hidden_size, num_features)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># ChatGpt special to generate edge index for fully connected graph:</span>
</span></span><span class=line><span class=cl>        <span class=n>edge_index</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>combinations</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>arange</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>]),</span> <span class=n>with_replacement</span><span class=o>=</span><span class=kc>False</span><span class=p>)</span><span class=o>.</span><span class=n>t</span><span class=p>()</span><span class=o>.</span><span class=n>contiguous</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>edge_index</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>edge_index</span><span class=p>,</span> <span class=n>edge_index</span><span class=o>.</span><span class=n>flip</span><span class=p>(</span><span class=mi>0</span><span class=p>)),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>edge_index</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv1</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>edge_index</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv2</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>edge_index</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>But this totally fails. I then backed things off a a bit to an even simpler case. If that doesn&rsquo;t work, then perhaps a hardcoded 3 body problem would:</p><a href=#fully-connected-mlp><h3 id=fully-connected-mlp><span class=hanchor arialabel=Anchor># </span>Fully connected MLP</h3></a><p>So I put together this. Obviously it would not generalise to more particles, but nonetheless you would think it would works as a hardcoded example. No, though. it&rsquo;s terrible.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>NbodyManual</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>num_features</span><span class=p>:</span> <span class=nb>int</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>500</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>num_features</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv1</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv2</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>num_features</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>orig_shape</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>x</span><span class=o>.</span><span class=n>ndim</span> <span class=o>==</span> <span class=mi>2</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>flatten</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>x</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span> <span class=o>-</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv1</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv2</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>relu</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=n>x</span><span class=o>.</span><span class=n>reshape</span><span class=p>(</span><span class=n>orig_shape</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>Here is what it looks like:
<img src="/quartz/Screencast from 06-04-2023 080721 AM.webm" width=auto>
Yuck.
The way that I trained things was by generating 10e3 trajectories of 1e3 timesteps each, and then randomly sampling from these at training time. For the MLP I also used a batch size of 1000 and learning rate of 3e-4. I messed about for many of these parameters but nothing changed anything. Even having fewer layers didn&rsquo;t.</p><a href=#go-simpler---y--x2><h1 id=go-simpler---y--x2><span class=hanchor arialabel=Anchor># </span>Go simpler - y = x**2</h1></a><p>Here is a neural net that is trying to do y = x**2:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>10</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=mi>1</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>act</span> <span class=o>=</span> <span class=n>LeakyReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>act</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>act</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>hidden_conv</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>And here are the training results:
<img src="/quartz/Pasted image 20230604100237.png" width=auto>
Here the vertical line denotes the limits of the training data. It&rsquo;s pretty clear from looking at the residual that the output is a piecewise linear approximation of the input. This is all very well and good but as expected it generalises super poorly outside the training distribution, which we can also see.
The reason that this is the case is of course that multiplications + relu cannot take the input of a neural net and multiply it by itself - This would enable it to learn the true function being approximated here. When simulating physics and stuff there are many types of relationships (like gravitation) where the relationship is a very simple one when expressed as multiplications and whatnot. But something like this can really only do linear approximations, and so it will never generalise very well.
It&rsquo;s been pointed out to me that one of the things you can do here is perform various interesting functions, x**n, sin(x), exp(x) etc etc as inputs to the net so that it can learn how they work.</p><a href=#the-bitter-lesson><h2 id=the-bitter-lesson><span class=hanchor arialabel=Anchor># </span>The bitter lesson</h2></a><p><a href=http://www.incompleteideas.net/IncIdeas/BitterLesson.html rel=noopener>The bitter lesson</a> is that all attempts to add human knowlege into the system are irrelevent compared to adding moar weights. The question is whether or not adding these fancy functions as inputs counts as this kind of optimisation. I suspect this comes down to how easy they are to approximate in practice. If you can do the job of an exp(x) with another layer or two it won&rsquo;t matter. But if you can&rsquo;t, then it does matter.
Another problem with such things are exploding gradients. 1/x, exp(x) etc etc all have this habit of going to infinity. Some very initial results here suggest that this is going to be a problem, since the opposite of the vanishing gradient problem will occur. The optimiser will spend all its time making sure that the exponential doesn&rsquo;t so anything funny and output 1e12 all of a sudden that it cannot actually approximate the desired function.</p><a href=#small-experiments-adding-spiciness><h3 id=small-experiments-adding-spiciness><span class=hanchor arialabel=Anchor># </span>Small experiments adding spiciness</h3></a><p>Adding x**2 to the input of the &ldquo;mlp&rdquo; so all it has to do is select the right input gives us this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=mi>2</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>act</span> <span class=o>=</span> <span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>act</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>And here is the result:
<img src="/quartz/Pasted image 20230604110559.png" width=auto>
So if you constrain stuff enough, things do generalise but of course the model has no way to know that the Relu kicking in one the right hand side will cause things not to work when it generalises.</p><a href=#muddying-the-waters><h4 id=muddying-the-waters><span class=hanchor arialabel=Anchor># </span>Muddying the waters</h4></a><p>Here is what happens when I add an exp(x) as an input to the network:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>class</span> <span class=nc>Net</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>nn</span><span class=o>.</span><span class=n>Module</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=fm>__init__</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=nb>super</span><span class=p>()</span><span class=o>.</span><span class=fm>__init__</span><span class=p>()</span>
</span></span><span class=line><span class=cl>            <span class=n>hidden_size</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=mi>3</span><span class=p>,</span> <span class=n>hidden_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span> <span class=o>=</span> <span class=n>Linear</span><span class=p>(</span><span class=n>hidden_size</span><span class=p>,</span> <span class=mi>1</span><span class=p>)</span> 
</span></span><span class=line><span class=cl>            <span class=bp>self</span><span class=o>.</span><span class=n>act</span> <span class=o>=</span> <span class=n>ReLU</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>forward</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>x</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>cat</span><span class=p>((</span><span class=n>x</span><span class=p>,</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>,</span> <span class=n>torch</span><span class=o>.</span><span class=n>exp</span><span class=p>(</span><span class=n>x</span><span class=p>)),</span> <span class=n>dim</span><span class=o>=</span><span class=mi>1</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>act</span><span class=p>(</span><span class=bp>self</span><span class=o>.</span><span class=n>input_conv</span><span class=p>(</span><span class=n>x</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>x</span> <span class=o>=</span> <span class=bp>self</span><span class=o>.</span><span class=n>output_conv</span><span class=p>(</span><span class=n>x</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span> <span class=n>x</span>
</span></span></code></pre></td></tr></table></div></div><p>Note that I also messed about with the hidden size etc to get the result that I wanted, as is
<a href=https://xkcd.com/1838/ rel=noopener>standard practice</a> for ML.
Here is the result:
<img src="/quartz/Pasted image 20230604111110.png" width=auto>
we can see that the loss absolutely explodes after a little bit. This is the effect I was looking for <a href=/quartz#the-bitter-lesson rel=noopener class=internal-link data-src=/quartz>before</a> and I think it will mean that adding this kind of stuff will cause problems.</p><a href=#inverse><h3 id=inverse><span class=hanchor arialabel=Anchor># </span>Inverse</h3></a><p>Here are the results of trying to simulate this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>fn</span><span class=p>(</span><span class=n>x</span><span class=p>):</span> <span class=k>return</span> <span class=mi>1</span> <span class=o>/</span> <span class=p>(</span><span class=mf>1e-3</span> <span class=o>+</span> <span class=n>x</span><span class=o>**</span><span class=mi>2</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>With an x2 available on the input:
<img src="/quartz/Pasted image 20230604115435.png" width=auto>
without:
<img src="/quartz/Pasted image 20230604115341.png" width=auto>
without, but with a single hidden layer:
<img src="/quartz/Pasted image 20230604120024.png" width=auto>
I suppose this experiment is a point in favour of the universal approximation theorum.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz// data-ctx="20230604 Simulate trivialities.md" data-src=/ class=internal-link>Harry dB notes</a></li><li><a href=/quartz/20230604-N-body-particulars/ data-ctx=this data-src=/20230604-N-body-particulars class=internal-link></a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://Oscilllator.github.io/quartz/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Harry dB using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2024</p><ul><li><a href=https://Oscilllator.github.io/quartz/>Home</a></li></ul></footer></div></div></body></html>