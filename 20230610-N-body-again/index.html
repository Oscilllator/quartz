<!doctype html><html lang=en><head><meta charset=utf-8><meta name=description content="[[20230610 inv x squared a deep dive|Previously, on NN adventures:]] I figured out that optimising the ratio of the input to the output was what was needed."><title>Harry dB notes</title><meta name=viewport content="width=device-width,initial-scale=1"><link rel="shortcut icon" type=image/png href=https://Oscilllator.github.io/quartz//icon.png><link href=https://Oscilllator.github.io/quartz/styles.708c2658f93e3a9d323a1f9fded8f4b2.min.css rel=stylesheet><link href=https://Oscilllator.github.io/quartz/styles/_light_syntax.86a48a52faebeaaf42158b72922b1c90.min.css rel=stylesheet id=theme-link><script src=https://Oscilllator.github.io/quartz/js/darkmode.9b46d81b9b161bfac149d66dfa6b3812.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/util.9825137f5e7825e8553c68ce39ac9e44.min.js></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css integrity=sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js integrity=sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.16.2/dist/contrib/copy-tex.min.js integrity=sha384-ww/583aHhxWkz5DEVn6OKtNiIaLi2iBRNZXfJRiY1Ai7tnJ9UXpEsyvOITVpTl4A crossorigin=anonymous></script>
<script src=https://unpkg.com/@floating-ui/core@0.7.3></script>
<script src=https://unpkg.com/@floating-ui/dom@0.5.4></script>
<script src=https://Oscilllator.github.io/quartz/js/popover.9b72b70bd35617d0635e9d15463662b2.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/code-title.b35124ad8db0ba37162b886afb711cbc.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/clipboard.c20857734e53a3fb733b7443879efa61.min.js></script>
<script src=https://Oscilllator.github.io/quartz/js/callouts.7723cac461d613d118ee8bb8216b9838.min.js></script>
<script>const BASE_URL="https://Oscilllator.github.io/quartz/",fetchData=Promise.all([fetch("https://Oscilllator.github.io/quartz/indices/linkIndex.1e0902e125c6bb61302c69bc89a21f8e.min.json").then(e=>e.json()).then(e=>({index:e.index,links:e.links})),fetch("https://Oscilllator.github.io/quartz/indices/contentIndex.854049b31210c1f6b2a5144076c61b1f.min.json").then(e=>e.json())]).then(([{index:e,links:t},n])=>({index:e,links:t,content:n})),render=()=>{const e=new URL(BASE_URL),t=e.pathname,n=window.location.pathname,s=t==n;addCopyButtons(),addTitleToCodeBlocks(),addCollapsibleCallouts(),initPopover("https://Oscilllator.github.io/quartz",!0,!0);const o=document.getElementById("footer");if(o){const e=document.getElementById("graph-container");if(!e)return requestAnimationFrame(render);e.textContent="";const t=s&&!1;drawGraph("https://Oscilllator.github.io/quartz",t,[{"/moc":"#4388cc"}],t?{centerForce:1,depth:-1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.5,linkDistance:1,opacityScale:3,repelForce:1,scale:1.4}:{centerForce:1,depth:1,enableDrag:!0,enableLegend:!1,enableZoom:!0,fontSize:.6,linkDistance:1,opacityScale:3,repelForce:2,scale:1.2})}},init=(e=document)=>{addCopyButtons(),addTitleToCodeBlocks(),renderMathInElement(e.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1}],macros:{'â€™':"'"},throwOnError:!1})}</script><script type=module>
    import { attachSPARouting } from "https:\/\/Oscilllator.github.io\/quartz\/js\/router.9d4974281069e9ebb189f642ae1e3ca2.min.js"
    attachSPARouting(init, render)
  </script></head><script async src="https://www.googletagmanager.com/gtag/js?id=G-XYFD95KB4J"></script>
<script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-XYFD95KB4J",{anonymize_ip:!1})}</script><body><div id=search-container><div id=search-space><input autocomplete=off id=search-bar name=search type=text aria-label=Search placeholder="Search for something..."><div id=results-container></div></div></div><script src=https://cdn.jsdelivr.net/npm/flexsearch@0.7.21/dist/flexsearch.bundle.js integrity="sha256-i3A0NZGkhsKjVMzFxv3ksk0DZh3aXqu0l49Bbh0MdjE=" crossorigin=anonymous defer></script>
<script defer src=https://Oscilllator.github.io/quartz/js/full-text-search.24827f874defbbc6d529926cbfcfb493.min.js></script><div class=singlePage><header><h1 id=page-title><a href=https://Oscilllator.github.io/quartz/>Harry dB notes</a></h1><div class=spacer></div><div id=search-icon><p>Search</p><svg tabindex="0" aria-labelledby="title desc" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 19.9 19.7"><title id="title">Search Icon</title><desc id="desc">Icon to open search</desc><g class="search-path" fill="none"><path stroke-linecap="square" d="M18.5 18.3l-5.4-5.4"/><circle cx="8" cy="8" r="7"/></g></svg></div><div class=darkmode><input class=toggle id=darkmode-toggle type=checkbox tabindex=-1>
<label id=toggle-label-light for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="dayIcon" viewBox="0 0 35 35" style="enable-background:new 0 0 35 35"><title>Light Mode</title><path d="M6 17.5C6 16.672 5.328 16 4.5 16h-3C.672 16 0 16.672.0 17.5S.672 19 1.5 19h3C5.328 19 6 18.328 6 17.5zM7.5 26c-.414.0-.789.168-1.061.439l-2 2C4.168 28.711 4 29.086 4 29.5 4 30.328 4.671 31 5.5 31c.414.0.789-.168 1.06-.44l2-2C8.832 28.289 9 27.914 9 27.5 9 26.672 8.329 26 7.5 26zm10-20C18.329 6 19 5.328 19 4.5v-3C19 .672 18.329.0 17.5.0S16 .672 16 1.5v3C16 5.328 16.671 6 17.5 6zm10 3c.414.0.789-.168 1.06-.439l2-2C30.832 6.289 31 5.914 31 5.5 31 4.672 30.329 4 29.5 4c-.414.0-.789.168-1.061.44l-2 2C26.168 6.711 26 7.086 26 7.5 26 8.328 26.671 9 27.5 9zM6.439 8.561C6.711 8.832 7.086 9 7.5 9 8.328 9 9 8.328 9 7.5c0-.414-.168-.789-.439-1.061l-2-2C6.289 4.168 5.914 4 5.5 4 4.672 4 4 4.672 4 5.5c0 .414.168.789.439 1.06l2 2.001zM33.5 16h-3c-.828.0-1.5.672-1.5 1.5s.672 1.5 1.5 1.5h3c.828.0 1.5-.672 1.5-1.5S34.328 16 33.5 16zM28.561 26.439C28.289 26.168 27.914 26 27.5 26c-.828.0-1.5.672-1.5 1.5.0.414.168.789.439 1.06l2 2C28.711 30.832 29.086 31 29.5 31c.828.0 1.5-.672 1.5-1.5.0-.414-.168-.789-.439-1.061l-2-2zM17.5 29c-.829.0-1.5.672-1.5 1.5v3c0 .828.671 1.5 1.5 1.5s1.5-.672 1.5-1.5v-3C19 29.672 18.329 29 17.5 29zm0-22C11.71 7 7 11.71 7 17.5S11.71 28 17.5 28 28 23.29 28 17.5 23.29 7 17.5 7zm0 18c-4.136.0-7.5-3.364-7.5-7.5s3.364-7.5 7.5-7.5 7.5 3.364 7.5 7.5S21.636 25 17.5 25z"/></svg></label><label id=toggle-label-dark for=darkmode-toggle tabindex=-1><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" id="nightIcon" viewBox="0 0 100 100" style="enable-background='new 0 0 100 100'"><title>Dark Mode</title><path d="M96.76 66.458c-.853-.852-2.15-1.064-3.23-.534-6.063 2.991-12.858 4.571-19.655 4.571C62.022 70.495 50.88 65.88 42.5 57.5 29.043 44.043 25.658 23.536 34.076 6.47c.532-1.08.318-2.379-.534-3.23-.851-.852-2.15-1.064-3.23-.534-4.918 2.427-9.375 5.619-13.246 9.491-9.447 9.447-14.65 22.008-14.65 35.369.0 13.36 5.203 25.921 14.65 35.368s22.008 14.65 35.368 14.65c13.361.0 25.921-5.203 35.369-14.65 3.872-3.871 7.064-8.328 9.491-13.246C97.826 68.608 97.611 67.309 96.76 66.458z"/></svg></label></div></header><article><p class=meta>Last updated
Unknown
<a href=https://github.com/jackyzha0/quartz/tree/hugo/content/20230610%20N%20body%20again.md rel=noopener>Edit Source</a></p><ul class=tags></ul><aside class=mainTOC><details><summary>Table of Contents</summary><nav id=TableOfContents><ol><li><ol><li><a href=#a-note-on-training-speed>A note on training speed.</a></li><li><a href=#first-results-with-new-fitness-function>First results with new fitness function:</a></li><li><a href=#quick-replication-attempt>Quick replication attempt.</a></li></ol></li><li><a href=#next-steps>Next steps</a></li><li><a href=#train-on-10-timesteps-of-one-simulation>Train on 10 timesteps of one simulation.</a></li><li><a href=#train-with-a-different-fitness-function>Train with a different fitness function.</a><ol><li><a href=#quick-experiment-batch-normalisation>Quick experiment: batch normalisation.</a></li></ol></li><li><a href=#exploring-the-error>Exploring the error</a></li><li><a href=#bringing-the-two-training-notions-together>Bringing the two training notions together</a></li><li><a href=#results>Results</a></li><li><a href=#what-if-we-did-the-opposite>What if we did the opposite?</a></li></ol></nav></details></aside><p><a href=/quartz/20230610-inv-x-squared-a-deep-dive rel=noopener class=internal-link data-src=/quartz/20230610-inv-x-squared-a-deep-dive>Previously, on NN adventures:</a> I figured out that optimising the ratio of the input to the output was what was needed. Let&rsquo;s use that information and go back to training N body simulations. Taking the <a href=/quartz/20230610-inv-x-squared-a-deep-dive#different-loss-function rel=noopener class=internal-link data-src=/quartz/20230610-inv-x-squared-a-deep-dive>Exact same</a> loss function and whacking it into the N body training loop (the one where we are just trying to pass the input to the output), the loss looks like this:
<img src="/quartz/Pasted image 20230610094721.png" width=auto>
Not great. Stuff just flies around in the sim as per usual, too. Looks like the learning rate is too high though.
Changing things so there is only one hidden layer and reducing the learning rate to 3e-5 and I get this:
<img src="/quartz/Pasted image 20230610095423.png" width=auto>
&mldr;Now that&rsquo;s more promising. But no, stuff just flies around in the simulator as per normal. I think I could do with a scheduler here. That took quite a few minutes to train, and at the end there it looked like the learning rate was still too high.
Here&rsquo;s the scheduler:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>    <span class=n>batch_size</span> <span class=o>=</span> <span class=mi>5000</span>
</span></span><span class=line><span class=cl>    <span class=n>oom_decay</span> <span class=o>=</span> <span class=mi>3</span>
</span></span><span class=line><span class=cl>    <span class=n>epochs</span> <span class=o>=</span> <span class=mi>10000</span>
</span></span><span class=line><span class=cl>    <span class=n>learning_rate_start</span> <span class=o>=</span> <span class=mf>1e-2</span>
</span></span><span class=line><span class=cl>    <span class=n>optimizer</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>optim</span><span class=o>.</span><span class=n>Adam</span><span class=p>(</span><span class=n>model</span><span class=o>.</span><span class=n>parameters</span><span class=p>(),</span> <span class=n>lr</span><span class=o>=</span><span class=n>learning_rate_start</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>scheduler</span> <span class=o>=</span> <span class=n>ExponentialLR</span><span class=p>(</span><span class=n>optimizer</span><span class=p>,</span> <span class=n>gamma</span><span class=o>=</span><span class=n>np</span><span class=o>.</span><span class=n>power</span><span class=p>(</span><span class=mi>10</span><span class=p>,</span> <span class=o>-</span><span class=n>oom_decay</span> <span class=o>/</span> <span class=n>epochs</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p>And the results:
<img src="/quartz/Pasted image 20230610100043.png" width=auto>
Boom. 2 OOM improvement in the loss with 1/6 the training time. Noice.
In the simulation we can actually see that the bodies kind of interact with each other. I noticed in the above graph things kinda slowed down a bunch at the end there, so trained for 10x longer with the same scheduler. And I got this:
<img src="/quartz/Pasted image 20230610100902.png" width=auto>
HMMMMMM. That is some nice perfectly scheduled learning right there (I think). Another 3 OOM improvement. Let&rsquo;s take a look in the simulator:
<img src="/quartz/Screencast from 06-10-2023 101107 AM.webm" width=auto>
!!!
Finally. Some modicum of success. Time to remove the output from the input lol.</p><a href=#a-note-on-training-speed><h3 id=a-note-on-training-speed><span class=hanchor arialabel=Anchor># </span>A note on training speed.</h3></a><p>It look 100e3 iterations of a batch size of 4e3 for this model with like 1000 parameters in it to learn the identity function. This really really does not bode well for learning anything more complex with short development cycles.</p><a href=#first-results-with-new-fitness-function><h3 id=first-results-with-new-fitness-function><span class=hanchor arialabel=Anchor># </span>First results with new fitness function:</h3></a><p>Here is what happens training over 1e6 iterations using a learning rate decaying from 1e-3->1e-6:
<img src="/quartz/Pasted image 20230610185747.png" width=auto>
&mldr;Yep, not great. This is quite similar to <a href=/quartz/20230604-N-body-particulars#ldquoquickrdquo-experiment-maybe-we-just-need-more-flops rel=noopener class=internal-link data-src=/quartz/20230604-N-body-particulars>what I got before</a> really (note of course that the absolute magnitude of the loss here is not comparable).</p><a href=#gradient-speed><h4 id=gradient-speed><span class=hanchor arialabel=Anchor># </span>Gradient speed</h4></a><p>Maybe what&rsquo;s going on here is that although we are optimising for the right thing here (percentage error) we are doing so in a way that produces a super weak gradient on account of the log. This is a bit of a incoherent notion that I don&rsquo;t really understand.</p><ul><li>If the gradient is weak, why not just multiply it by a higher learning rate? Perhaps because the variance in the model is not correspondingly weak, and so the signal to noise ratio is poor here?</li><li>If we have a fitness function with a stronger gradient, isn&rsquo;t that basically the same as going back to mean squared error again? Is it a linear thing between &ldquo;correct fitness function that takes ratio but gives weak signal&rdquo; and &ldquo;strong fitness function that gives good signal but prioritises fixing spiky bits in the loss landscape at the expense of everything else&rdquo;, or are the two independent and we can find a function that performs well one both accounts?
I don&rsquo;t really know what if any of these things are true. Maybe this is where one of those literature searches would be good. One of my favourite papers &ldquo;Learning to simulate complex physics with graph neural networks&rdquo; has this to say on the topic:
<img src="/quartz/Pasted image 20230610190612.png" width=auto>
They were doing fluid simulations and suchlike. I think though that what they were doing might actually be a bit easier than this though. Intuitively I would expect the dynamic range of the problem to be quite a bit lower. Water particles only interact with what&rsquo;s right next to them and do so with a (relatively) low range of forces. So they might have just not run into this problem.</li></ul><a href=#quick-replication-attempt><h3 id=quick-replication-attempt><span class=hanchor arialabel=Anchor># </span>Quick replication attempt.</h3></a><p><a href=https://arxiv.org/pdf/1910.07291.pdf rel=noopener>here</a> is a paper that does the three body problem - exactly the same as what I have been trying to do. Prolly should have looked this up earlier but oh well. Their structure is also super simple - a 128 wide by 10 deep MLP. let&rsquo;s whack that into pytorch and see how it goes.
Here is the loss function:
<img src="/quartz/Pasted image 20230610211605.png" width=auto>
Gyarbage! We know from above that you need like 1e-6->1e-8 loss to get good results, this isn&rsquo;t even close to that. I&rsquo;m getting pretty strong &ldquo;dataset is high variance and that&rsquo;s why it isn&rsquo;t training&rdquo; vibes from that loss function too.</p><a href=#next-steps><h2 id=next-steps><span class=hanchor arialabel=Anchor># </span>Next steps</h2></a><p>So from here I can:</p><ul><li>Fudge the N body simulation so that it&rsquo;s smoother, akin to having an epsilon of 1e-2 <a href=/quartz/20230610-inv-x-squared-a-deep-dive#background rel=noopener class=internal-link data-src=/quartz/20230610-inv-x-squared-a-deep-dive>like this</a> to see if it makes things easier to train. This will verify that it&rsquo;s the spikyness that is what&rsquo;s causing the issue.</li><li>Try to find a better loss function, since I got such great improvements from that route already</li><li>Train on a tiny subset of the data to try to overfit, and then generalize from there.</li></ul><p>The last option is easiest, so let&rsquo;s try that next:</p><a href=#train-on-10-timesteps-of-one-simulation><h2 id=train-on-10-timesteps-of-one-simulation><span class=hanchor arialabel=Anchor># </span>Train on 10 timesteps of one simulation.</h2></a><p>Here is the loss curve for training on 10 simulation steps of a single initial state only:
<img src="/quartz/Pasted image 20230610215854.png" width=auto>
So that seems somewhat reasonable. Let&rsquo;s bump it up to 10 trajectories each with 10 simulation steps:
<img src="/quartz/Pasted image 20230610220101.png" width=auto>
So we can see that already that&rsquo;s enough to cook the loss!</p><a href=#train-with-a-different-fitness-function><h2 id=train-with-a-different-fitness-function><span class=hanchor arialabel=Anchor># </span>Train with a different fitness function.</h2></a><p>I updated the fitness function to the one <a href=/quartz/20230610-inv-x-squared-a-deep-dive#going-deeper rel=noopener class=internal-link data-src=/quartz/20230610-inv-x-squared-a-deep-dive>here</a>, and then trained it on the 128 wide 10 deep architecture from before for about 8 hours with a learning rate decaying from 3e-3 to 2e-6.
Dataset was <em>two</em> bodies this time, 50e3 trajectories of 1000 timesteps each.
Here are the results:
<img src="/quartz/Pasted image 20230612172952.png" width=auto>
well, it seems to have converged on something&mldr;
Let&rsquo;s check it out in the sim:
<img src="/quartz/Screencast from 06-12-2023 053202 PM.webm" width=auto>
It verks!
Incredible. It doesn&rsquo;t work that well, but it does recognisably solve the problem. Amazing. Only took like a month. Now to do the same thing, but with <em>three</em> bodies.
And here are the results for that:
<img src="/quartz/Pasted image 20230613080604.png" width=auto>
&mldr;Seems to converge with an error about 10x higher, oh well. Let&rsquo;s zoom in on one of these spikes of error:
<img src="/quartz/Pasted image 20230613080704.png" width=auto>
so it looks like the model sometimes gets updated in a very poor direction, then has to spend a long time recovering from this before it has a chance to get back to where it was. Even though the batch size is like 8000, I suspect that the cause of this is one example within the batch having a truly stupendous error that throws everything out. I guess this could be from two causes:</p><ul><li>Early in the training process the model has not adapted well, and so occasionally guesses very wrong.</li><li>Early in the training process the learning rate is high, so the model weights get thrown into a bad area of the loss function.
The latter sounds more likely to me. If this is the failure mode of the model however, one thing that we could do is do something like</li></ul><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>for</span> <span class=n>epoch</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>num_epochs</span><span class=p>):</span>
</span></span><span class=line><span class=cl>	<span class=n>model</span><span class=p>(</span><span class=n>data</span><span class=p>)</span><span class=o>...</span><span class=n>blah</span> <span class=n>blah</span> <span class=n>train</span>
</span></span><span class=line><span class=cl>	<span class=k>if</span><span class=p>(</span><span class=n>curr_loss</span> <span class=o>&gt;</span> <span class=n>prev_loss</span> <span class=o>*</span> <span class=mi>10</span><span class=p>):</span> <span class=k>continue</span>
</span></span></code></pre></td></tr></table></div></div><p>This might be a good idea but I think it is also papering over the issue. Before I go and do something like that we should go and understand in great and excruciating detail why it is that everything is so profoundly fat tailed. I think that this is most likely a lesson that will translate well into future efforts.
For example if our ground truth is a simulation, maybe we can gradually increase the &lsquo;peakiness&rsquo; of the training data as a function of time (i.e. gradually decrease epsilon <a href=/quartz/20230610-inv-x-squared-a-deep-dive rel=noopener class=internal-link data-src=/quartz/20230610-inv-x-squared-a-deep-dive>like I did here</a>) which might make things train faster. That seems like something that could generalise well. It also seems like something that could paint you into a corner where you could only train on data that could be smoothed in this way too.
GPT tells me that this kind of thing is called &lsquo;Curriculum learning&rsquo;.</p><a href=#quick-experiment-batch-normalisation><h3 id=quick-experiment-batch-normalisation><span class=hanchor arialabel=Anchor># </span>Quick experiment: batch normalisation.</h3></a><p>Here is the result of the same 10 layer network, only the first 7 layers are batch norm:
<img src="/quartz/Pasted image 20230613204142.png" width=auto>
basically exactly the same (take note of x axis).</p><a href=#exploring-the-error><h2 id=exploring-the-error><span class=hanchor arialabel=Anchor># </span>Exploring the error</h2></a><p>I set a breakpoint in vscode for when the error went up by more than a factor of 100, and got a breakpoint around 300e3. Here is what the ratio losses look like across the batch size of 8000:
<img src="/quartz/Pasted image 20230613205906.png" width=auto>
What kind of absolutely atrocious distribution is that?
Here are what things look like for the fitness function before that, the <a href=/quartz/20230610-inv-x-squared-a-deep-dive#381054 rel=noopener class=internal-link data-src=/quartz/20230610-inv-x-squared-a-deep-dive>two sided log</a>:
<img src="/quartz/Pasted image 20230613210249.png" width=auto>
well I&rsquo;m not going to say that&rsquo;s great, but it looks much better than the previous one. It&rsquo;s clear with this too though that the loss is dominated by these outliers. inspecting them, they seem to be important as this is where the n bodies are undergoing high accelerations. Maybe if we optimised for the 0->95th percentile of losses this would work out better though and even though we would not be training on such extreme cases the model would still learn better. So many experiments!</p><a href=#bringing-the-two-training-notions-together><h2 id=bringing-the-two-training-notions-together><span class=hanchor arialabel=Anchor># </span>Bringing the two training notions together</h2></a><p>I have had two ideas about how to train these spiky functions:</p><ul><li>Gradually increase the spikiness of the training data over time by increasing the epsilon in 1/x^2 or similar.</li><li>Reject some outlier losses to stop egregious model updates
But don&rsquo;t you see! Those are the same! If we we gradually update the weights over the training period from the 0th to 100th percentile of the losses, then it will by definition be learning on the easiest (smoothest) examples first!
Someone must have thought of this already&mldr;</li></ul><a href=#results><h2 id=results><span class=hanchor arialabel=Anchor># </span>Results</h2></a><p>Here is the loss curve from training in such a way that over the course of the training set the loss is calculated from a gradually increasing fraction of the errors, like so:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span><span class=lnt>7
</span><span class=lnt>8
</span><span class=lnt>9
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>        <span class=k>def</span> <span class=nf>my_loss</span><span class=p>(</span><span class=n>target</span><span class=p>,</span> <span class=n>pred</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>min_batch_ratio</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>            <span class=n>percentile</span> <span class=o>=</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>/</span> <span class=n>epochs</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>min_batch_ratio</span><span class=p>)</span> <span class=o>+</span> <span class=n>min_batch_ratio</span>
</span></span><span class=line><span class=cl>            <span class=n>ratio</span> <span class=o>=</span> <span class=p>(</span><span class=n>pred</span> <span class=o>-</span> <span class=n>target</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span><span class=mi>1</span> <span class=o>+</span> <span class=n>target</span><span class=o>.</span><span class=n>abs</span><span class=p>())</span>
</span></span><span class=line><span class=cl>            <span class=n>ratio_batch</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>mean</span><span class=p>(</span><span class=n>ratio</span><span class=p>,</span> <span class=n>dim</span><span class=o>=</span><span class=p>(</span><span class=o>-</span><span class=mi>1</span><span class=p>,</span> <span class=o>-</span><span class=mi>2</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>ratio_percentile</span> <span class=o>=</span> <span class=n>torch</span><span class=o>.</span><span class=n>quantile</span><span class=p>(</span><span class=n>torch</span><span class=o>.</span><span class=n>abs</span><span class=p>(</span><span class=n>ratio_batch</span><span class=p>),</span> <span class=n>percentile</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>ratio_batch</span> <span class=o>=</span> <span class=n>ratio_batch</span><span class=p>[</span><span class=n>ratio_batch</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span> <span class=o>&lt;</span> <span class=n>ratio_percentile</span><span class=p>]</span>
</span></span><span class=line><span class=cl>            <span class=k>return</span>  <span class=n>ratio_batch</span><span class=o>.</span><span class=n>abs</span><span class=p>()</span><span class=o>.</span><span class=n>mean</span><span class=p>()</span>
</span></span><span class=line><span class=cl>        <span class=n>loss</span> <span class=o>=</span> <span class=n>F</span><span class=o>.</span><span class=n>l1_loss</span><span class=p>(</span><span class=n>my_loss</span><span class=p>(</span><span class=n>data_end</span><span class=p>,</span> <span class=n>out</span><span class=p>),</span> <span class=n>torch</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=mi>1</span><span class=p>))</span>
</span></span></code></pre></td></tr></table></div></div><p><img src="/quartz/Pasted image 20230614075059.png" width=auto>
Well it certainly looks like something happened there. But in the simulation the bodies just fly apart. At least there were no spikes during training. Let&rsquo;s take a look at what happens with the exact same setup and loss function, only difference is percentile is set to 1 always:
<img src="/quartz/Pasted image 20230614075623.png" width=auto>
I suppose this isn&rsquo;t really valid since the first net never even trained at all on the hard examples really. So I changed the percentile calculation to this:</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl>            <span class=n>min_batch_ratio</span> <span class=o>=</span> <span class=mf>0.1</span>
</span></span><span class=line><span class=cl>            <span class=n>percentile</span> <span class=o>=</span> <span class=mi>2</span> <span class=o>*</span> <span class=p>(</span><span class=n>epoch</span> <span class=o>/</span> <span class=n>epochs</span><span class=p>)</span> <span class=o>*</span> <span class=p>(</span><span class=mi>1</span> <span class=o>-</span> <span class=n>min_batch_ratio</span><span class=p>)</span> <span class=o>+</span> <span class=n>min_batch_ratio</span>
</span></span><span class=line><span class=cl>            <span class=n>percentile</span> <span class=o>=</span> <span class=nb>min</span><span class=p>(</span><span class=n>percentile</span><span class=p>,</span> <span class=mf>1.0</span><span class=p>)</span>
</span></span></code></pre></td></tr></table></div></div><p>so that it would train on all the dataset for the last half of the training set.
<img src="/quartz/Pasted image 20230614080045.png" width=auto>
So the final loss here is actually quite a bit better than the no-curriculum alternative, it looks like.
The results in the simulation are a bit interesting, they looks like this:
<img src="/quartz/Screencast from 06-14-2023 080236 AM.webm" width=auto></p><a href=#what-if-we-did-the-opposite><h2 id=what-if-we-did-the-opposite><span class=hanchor arialabel=Anchor># </span>What if we did the opposite?</h2></a><p>Maybe the problem is not that the datset is super fat tailed, making it difficult to train on. Maybe the problem is that the training data is 99% &ldquo;objects in motion stay in motion&rdquo; and 1% &ldquo;actual gravitation&rdquo;.
So what if we trained the net on a dataset where the bodies where experiencing high accelerations?
Here is the result of training on the ~90th percentile examples with the highest accelerations (most newtons law, least &ldquo;objects in motion&rdquo;), learning rate decaying from 3e-3->3e-6.
<img src="/quartz/Pasted image 20230614220615.png" width=auto>
more garbage. I think I&rsquo;ll give up on nbodies for now, too hard. The whole thing was supposed to be a learning exercise anyway and I think it&rsquo;s run its course there.</p></article><hr><div class=page-end id=footer><div class=backlinks-container><h3>Backlinks</h3><ul class=backlinks><li><a href=/quartz// data-ctx="20230610 N body again.md" data-src=/ class=internal-link>Harry dB notes</a></li><li><a href=/quartz/20230610-inv-x-squared-a-deep-dive/ data-ctx="more troubles" data-src=/20230610-inv-x-squared-a-deep-dive class=internal-link></a></li><li><a href=/quartz/20230610-invx2-a-deep-dive/ data-ctx="more troubles" data-src=/20230610-invx2-a-deep-dive class=internal-link></a></li></ul></div><div><script src=https://cdn.jsdelivr.net/npm/d3@6.7.0/dist/d3.min.js integrity="sha256-+7jaYCp29O1JusNWHaYtgUn6EhuP0VaFuswhNV06MyI=" crossorigin=anonymous></script><h3>Interactive Graph</h3><div id=graph-container></div><style>:root{--g-node:var(--secondary);--g-node-active:var(--primary);--g-node-inactive:var(--visited);--g-link:var(--outlinegray);--g-link-active:#5a7282}</style><script src=https://Oscilllator.github.io/quartz/js/graph.abd4bc2af3869a96524d7d23b76152c7.js></script></div></div><div id=contact_buttons><footer><p>Made by Harry dB using <a href=https://github.com/jackyzha0/quartz>Quartz</a>, Â© 2024</p><ul><li><a href=https://Oscilllator.github.io/quartz/>Home</a></li></ul></footer></div></div></body></html>